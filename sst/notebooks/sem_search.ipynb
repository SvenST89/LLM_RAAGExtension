{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f72a287-9d2e-4fb5-9fb0-4403254d34e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\svens\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import torch\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a5c6925-5014-40db-b5d6-94cd066fda05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================#\n",
    "# SET LOGGING\n",
    "#================================================#\n",
    "logger = logging.getLogger()\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(lineno)s - %(levelname)s - %(message)s')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# set logging handler in file\n",
    "fileHandler = logging.FileHandler(filename=\"log/search.log\", mode=\"w\")\n",
    "fileHandler.setFormatter(formatter)\n",
    "fileHandler.setLevel(logging.DEBUG)\n",
    "logger.addHandler(fileHandler)\n",
    "\n",
    "# set logging handler in Console\n",
    "consoleHandler = logging.StreamHandler(sys.stdout)\n",
    "consoleHandler.setFormatter(formatter)\n",
    "consoleHandler.setLevel(logging.ERROR)\n",
    "logger.addHandler(consoleHandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03c0c2c5-f8a7-40d2-a6b5-6170b2725793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSearch:\n",
    "    #################################################\n",
    "    # COMPUTING SETUP\n",
    "    #################################################\n",
    "    # https://thegeeksdiary.com/2023/03/23/how-to-set-up-pytorch-with-gpu-support-on-windows-11-a-comprehensive-guide/\n",
    "    # Check if PyTorch uses the GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"PyTorch is using: {device}\")\n",
    "    \n",
    "    # print Torch version\n",
    "    logging.info(f\"PyTorch version: {torch.__version__}\")\n",
    "    \n",
    "    #================================================#\n",
    "    # CLASS ATTRIBUTES. The same for each Class Object\n",
    "    #================================================#\n",
    "    model_path = \"02_DataScience/00_ML/NLP/openModels/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    model_path_rerank = \"\"\n",
    "    \n",
    "    #================================================#\n",
    "    # INSTANCE ATTRIBUTES. The same for each Class Object\n",
    "    #================================================#\n",
    "    def __init__(self, query, top_k_sent, corpus: list):\n",
    "        # check if corpus is list of sentences\n",
    "        if not isinstance(corpus, list):\n",
    "            raise TypeError(\"Corpus is not a list of passages/sentences!\")\n",
    "            \n",
    "            self.query = query\n",
    "            self.top_k_sent = top_k_sent\n",
    "            self.corpus = corpus\n",
    "            \n",
    "            # Parameters Log\n",
    "            logging.info(\"##################################################\\n\")\n",
    "            logging.info(\"GIVEN PARAMETERS\\n---------------------------------------------------\\n\")\n",
    "            logging.info(f\"QUERY: {self.query}\")\n",
    "            logging.info(f\"NUMBER OF TOP RESULTS SHOWN: {self.query}\")\n",
    "            logging.info(f\"Corpus is up to you...\")\n",
    "            logging.info(\"##################################################\\n\")\n",
    "            \n",
    "    #================================================#\n",
    "    # METHODS\n",
    "    #================================================#\n",
    "    \n",
    "    #------------------------------------------------------------------------------#\n",
    "    # PROTECTED METHODS\n",
    "    #------------------------------------------------------------------------------#\n",
    "    # These are used inside this class within other methods, \n",
    "    # but is not used inside the main code on a class instance level.\n",
    "    # For building the embedder and the cross encoder we need to\n",
    "    # call the class \"SemanticSearch\" in order to address the class attributes.\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_embedder():\n",
    "        return SentenceTransformer(model_name_or_path=SemanticSearch.model_path, device = SemanticSearch.device)\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_CrossEncoder():\n",
    "        return CrossEncoder(SemanticSearch.model_path_rerank, device = SemanticSearch.device)\n",
    "    \n",
    "    #------------------------------------------------------------------------------#\n",
    "    # INSTANCE METHODS\n",
    "    #------------------------------------------------------------------------------#\n",
    "    def do_semantic_search(self):\n",
    "        logging.info(f\"\\n=============================================================\\nINPUT/QUERY: {self.query}\\n=============================================================\\n\")\n",
    "        \n",
    "        #================================#\n",
    "        # SEMANTIC SEARCH\n",
    "        #----------------------------------------------------#\n",
    "        embedder = self.build_embedder()\n",
    "        query_embedding = embedder.encode(self.query, convert_to_tensor=True, device = SemanticSearch.device)\n",
    "        corpus_embeddings = embedder.encode(self.corpus, convert_to_tensor=True, device=SemanticSearch.device)\n",
    "        # Über Cosine-Similarity abgleichen, wie gut query semantisch zum Corpus passt\n",
    "        cosine_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "        top_results = torch.topk(cosine_scores, k=self.top_k_sent) # liefert einen tensor mit \"values\" = \"score\" und \"indices\" sind die corpus ids.\n",
    "        # Die Ähnlichkeitsmetrik der query-corpus-vektoren befinden sich bei index '0' im tensor\n",
    "        # copy gpu-tensor first to CPU, then transform into numpy array.\n",
    "        scores = top_results[0].cpu().numpy()\n",
    "        # ID in corpus, index 1 in tensor\n",
    "        IDs = top_results[1].cpu().numpy()\n",
    "        hits = []\n",
    "        # mache list of dictionary, z. B. wie folgt: [{'corpus_id': 1, 'score':0.57}, {...}]\n",
    "        for score, idx in zip(scores, IDs):\n",
    "            top_dict = {}\n",
    "            top_dict['corpus_id'] = idx\n",
    "            top_dict['score'] = score\n",
    "            hits.append(top_dict)\n",
    "        \n",
    "        #================================#\n",
    "        # RERANKING OF RESULTS\n",
    "        #----------------------------------------------------#\n",
    "        cross_inp = [[self.query, self.corpus[hit['corpus_id']]] for hit in hits]\n",
    "        cross_encoder = self.build_CrossEncoder()\n",
    "        cross_scores = cross_encoder.predict(cross_inp)\n",
    "        \n",
    "        # Sort results by cross-encoder scores\n",
    "        for idx in range(len(cross_scores)):\n",
    "            hits[idx]['cross-score'] = cross_scores[idx]\n",
    "            \n",
    "        \n",
    "         #================================#\n",
    "        # OUTPUS\n",
    "        #----------------------------------------------------#\n",
    "        # Output of top hits from Semantic Encoder\n",
    "        logging.info(\"Top Hits from the SEMANTIC SEARCH MODEL.\")\n",
    "        logging.info(\"---------------------------------------------------------------------------\\n\")\n",
    "        hits_sem = sorted(hits, key=lambda x: x['score'], reverse=True)\n",
    "        result_list_sem = []\n",
    "        for hit in hits_sem[0: int(self.top_k_sent)]:\n",
    "            result_dict_sem = {}\n",
    "            result_dict_sem['corpus_passage'] = self.corpus[hit['corpus_id']]\n",
    "            result_dict_sem['score'] = hit['score']\n",
    "            result_list_sem.append(result_dict_sem)\n",
    "            logging.info(self.corpus[hit['corpus_id']], \"(Score: {:.4f})\".format(hit['score']))\n",
    "            \n",
    "        \n",
    "        # SEMANTIC RESULTS\n",
    "        result_df_sem = pd.DataFrame(result_list_sem)\n",
    "        result_df_sem_unique = result_df_sem.drop_duplicates() # check all columns and drop duplicate rows\n",
    "        result_df_sem_unique.to_csv(\"data/result_df_sem_unique.csv\", index = False, header = True)\n",
    "        \n",
    "        return result_df_sem_unique\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231904ac-7f9f-4bcc-9fe9-dfa4937885a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
