{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53ec5c1a-2ce9-461f-99f6-28fa6d78cb4e",
   "metadata": {},
   "source": [
    "# Notebook For LangChain Testing with Falcon 40b-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d4e31b-35fa-4292-b03a-144452a6c1d6",
   "metadata": {},
   "source": [
    "**Author:** <u>Sven STEINBAUER</u><br>\n",
    "**Date:** $\\underline{10^{th} Nov. 2023}$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb6886c-36ff-4ce1-9378-034a2c3bb8ce",
   "metadata": {},
   "source": [
    "## Necessary Imports / Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57cde6fe-7a76-4ae4-bb9d-fb75510d6320",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-11 12:33:23,377 - root - 33 - INFO - PyTorch is using: cuda\n",
      "2023-11-11 12:33:23,378 - root - 36 - INFO - PyTorch version: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "# System Libs\n",
    "import logging\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# NLP Libs\n",
    "# from ctransformers import AutoModelForCausalLM\n",
    "from langchain.llms import CTransformers # for falcon, llama and mistral models\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from accelerate import Accelerator # to put llm inference with ctransformers on my Nvidia GPU\n",
    "\n",
    "# NLP EDA Personal LIbs\n",
    "from sst.pdf_extract import PDFExtract\n",
    "from sst.ppt_extract import PPTExtract\n",
    "from sst.sem_search import SemanticSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1d8ec4-3d7e-43c3-b978-5a8216f463c4",
   "metadata": {},
   "source": [
    "## Make Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7981326d-a14e-4a5c-a9d9-eb9e8f8d8f39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#================================#\n",
    "# Build Corpus\n",
    "#================================#\n",
    "base_path = r\"C:\\Users\\svens\\00_Private\\Knowledge\\ECONOMICS-TheCode\\Keynes\"\n",
    "field = \"keynes\"\n",
    "lang = \"en\"\n",
    "# ----------------------------------------------------\n",
    "# PDF Alternative\n",
    "#pdf_reader = PDFExtract(base_path=base_path, field=field)\n",
    "\n",
    "# Write dictionary file as \"key\" and dictionary of pages and related content as \"value\", e.g. {\"file1-path\":{\"page1\":[text_content,..., page_content_general]...}, \"file2-path\":...}\n",
    "#keynes_dict = pdf_reader.write_pdf_corpus_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c060193e-a4eb-4b81-8348-095c7e86b732",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#if lang == \"de\":\n",
    "    #with open(f\"data/file_page_{field}_{lang}.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "        #json.dump(keynes_dict, outfile, ensure_ascii=False)\n",
    "#else:\n",
    "    #with open(f\"data/file_sentence_{field}_{lang}.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "       # json.dump(keynes_dict, outfile, ensure_ascii=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5935f59c-ca29-40d2-aad9-bbe2e7958db3",
   "metadata": {},
   "source": [
    "In my case I have already executed all the PDFs which I wanted to extract. I extracted around 400 pages+. This, unfortunately, took a while. Each run of this module will create a corpus text-file in the `data/` folder called `pdf_corpus_{field}.txt`. So, in order to speed up the code, I uncommented the execution of the **PDFExtract-Class** and directly work on the created text-file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058f355b-79a6-4786-bcad-7b11598f70ff",
   "metadata": {},
   "source": [
    "## EDA: Prepare Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f57fcafa-e790-4e57-9b74-4d8e6bf62ca3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For Cleaning content\n",
    "def chain_sub(input_string, substitutions):\n",
    "        # substitutions must be a list of tuples(r'expression_to_be_replaced', 'replacement_string')\n",
    "        for pattern, replacement in substitutions:\n",
    "            input_string = re.sub(pattern, replacement, input_string)\n",
    "        return input_string\n",
    "    \n",
    "# Regex Pattern to be replaced\n",
    "pat_and_replace=[\n",
    "    (r\"\\n\", \"\"),\n",
    "    (r\"[+*%?!'.;,@ʺ‎…`]\", ''),\n",
    "    (r\"(´)\", ''),\n",
    "    (r\"(\\$)\", 's'),\n",
    "    (r\"(<UNK>+)\", ''),\n",
    "    (r\"(ñ+)\", 'n'),\n",
    "    (r\"(\\\")\", ''),\n",
    "    (r\"(/)\", ' '),\n",
    "    (r\"(ş)\", 's'),\n",
    "    (r\"(ă)\", 'a'),\n",
    "    (r\"(†)\", ''),\n",
    "    (r\"(ŭ)\", 'u'),\n",
    "    (r\"(ŏ)\", 'o'),\n",
    "    (r\"(ō)\", 'o'),\n",
    "    (r\"(ī)\", 'i'),\n",
    "    (r\"(ç)\", 'c'),\n",
    "    # replace :-\n",
    "    (r'(:+|-+)', ' '),\n",
    "    # replace empty spaces with single space\n",
    "   (r\" +\", ' '),\n",
    "        ]\n",
    "\n",
    "corpus_path = \"data/pdf_corpus_keynes.txt\"\n",
    "lines_list=[]\n",
    "clean_content_file = open(f\"data/clean_corpus_{field}.txt\", \"w\", encoding=\"utf-8\")\n",
    "with open(corpus_path, encoding=\"utf-8\") as c:\n",
    "    for line in c:\n",
    "        _line = line.strip()\n",
    "        clean_line = chain_sub(_line, pat_and_replace)\n",
    "        lines_list.append(clean_line + \"\\n\")\n",
    "    for l in lines_list:\n",
    "        # ignore empty lines\n",
    "        if not l.isspace():\n",
    "            clean_content_file.write(l)\n",
    "clean_content_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d66a77-374e-4cc8-94fe-fe80469fb0a4",
   "metadata": {},
   "source": [
    "## Semantic Search with Torch & SentenceTransformer: Calculate Embeddings, Match Query & Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db492a5-a7f0-4337-8974-17acb63774c8",
   "metadata": {},
   "source": [
    "Once we have a clean corpus we can proceed by getting the corpus into a list of sentences which can be used for the `SentenceTransformer` Class in order to calculate embeddings of the corpus sentences and the query such that we can finally compary both embeddings via the **cosine similarity metric**, i.e. the vectors which have the smallest angle with each other.\n",
    "\n",
    "We want to implement this *similarity search* via the `SemanticSearch` Class which I have written.\n",
    "\n",
    "Specifically, we will receive basically a dataframe consisting of top-x number of entries that contain the respective passage of the corpus and the related score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10ec606c-9e16-4903-ac26-98f2ff35b9cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Please explain the theory of business cycles.\"\n",
    "top_k_sent = 5\n",
    "#===============================================#\n",
    "# TRANSFORM CORPUS ACCORDINGLY\n",
    "#===============================================#\n",
    "def get_list_of_sentences(file):\n",
    "    global string_list\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        corpus = f.read()\n",
    "        string_list = corpus.split(\"\\n\")\n",
    "    return string_list\n",
    "\n",
    "file = f\"data/clean_corpus_{field}.txt\"\n",
    "corpus = get_list_of_sentences(file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd8a1268-e5c9-4ff0-b1b3-87f031bad768",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-11 12:33:29,147 - root - 57 - INFO - ##################################################\n",
      "\n",
      "2023-11-11 12:33:29,148 - root - 58 - INFO - GIVEN PARAMETERS\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "2023-11-11 12:33:29,148 - root - 59 - INFO - QUERY: Please explain the theory of business cycles.\n",
      "2023-11-11 12:33:29,149 - root - 60 - INFO - NUMBER OF TOP RESULTS SHOWN: 5\n",
      "2023-11-11 12:33:29,150 - root - 61 - INFO - Corpus is up to you...\n",
      "2023-11-11 12:33:29,151 - root - 62 - INFO - ##################################################\n",
      "\n",
      "2023-11-11 12:33:29,151 - sentence_transformers.SentenceTransformer - 66 - INFO - Load pretrained SentenceTransformer: C:\\Users\\svens\\02_DataScience\\00_ML\\NLP\\openModels\\paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ad3ed974ff414787f06c9ef71ee425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3595fb02a74e0fb2d4af61035bfd31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-11 12:33:41,887 - root - 96 - INFO - Embeddings calculated...\n",
      "\n",
      "2023-11-11 12:33:41,906 - root - 129 - INFO - Top Hits from the SEMANTIC SEARCH MODEL.\n",
      "2023-11-11 12:33:41,906 - root - 130 - INFO - ---------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#===============================================#\n",
    "# DO SEMANTIC SEARCH AND RETURN BEST MATCHES\n",
    "#===============================================#\n",
    "sem_search = SemanticSearch(query=query, top_k_sent=top_k_sent, corpus=corpus)\n",
    "search_result = sem_search.do_semantic_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d32275e-b14e-4ab5-8de6-47cdc88cca89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cycles having a regular phase has been founded The same thing is true of prices which'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best match\n",
    "best_match = str(search_result['corpus_passage'].iloc[0])\n",
    "best_match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb85a71a-f1e7-4a52-8e4b-f6b8884dcddc",
   "metadata": {},
   "source": [
    "## Find Context in your Corpus based on Semantic Search Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d5cefca-3246-467c-8355-2446d47ed214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#===============================================#\n",
    "# FIND CORRESPONDING PAGE CONTENT RELATED TO BEST MATCH SENTENCE\n",
    "#===============================================#\n",
    "# show dict keys\n",
    "file_page_cont_dict = json.load(open(f\"data/file_page_{field}_{lang}.json\", \"r\", encoding=\"utf-8\"))\n",
    "# now get the right key = file-name\n",
    "# use the list.index()-method\n",
    "\n",
    "# All files collected; those are the keys\n",
    "key_list = list(file_page_cont_dict.keys())\n",
    "\n",
    "# All page dictionaries of all files\n",
    "page_dict_list = list(file_page_cont_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cac46362-c5be-447c-9e89-c15bb1272794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_context_data(best_match, key_list, file_page_cont_dict):\n",
    "    break_out_of_outer_loop_flag = False\n",
    "    found_strings = []\n",
    "    for file_key in key_list:\n",
    "        #print(file_key)\n",
    "        # get the page dictionary of the individual file.\n",
    "        pages_dictionary = file_page_cont_dict[file_key]\n",
    "        # now iterate through the pages (as keys) and extract the fourth element of the list of elements that belongs to that page-key;\n",
    "        # the fourth element holds the whole page_content as text\n",
    "        for page_key in list(pages_dictionary.keys()):\n",
    "            text = ''.join(pages_dictionary[page_key][4])\n",
    "            # important step: clean the text to remove the new-line characters '\\n'\n",
    "            clean_text = chain_sub(text, pat_and_replace)\n",
    "            # since Python 3.6 one can use an f-string to include variables in a regex together with a r-string (raw)\n",
    "            m = re.search(fr\"{re.escape(best_match)}\", clean_text, re.IGNORECASE)\n",
    "            if m:\n",
    "                logging.info(\"match found\")\n",
    "                found_strings.append(clean_text)\n",
    "                file = file_key\n",
    "                page_id = page_key\n",
    "                break_out_of_outer_loop_flag = True\n",
    "                break\n",
    "        if break_out_of_outer_loop_flag:\n",
    "            break\n",
    "    return found_strings, file, page_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e4ae97c-51ed-4c6d-b89d-30bc7015298c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-11 12:33:49,683 - root - 17 - INFO - match found\n"
     ]
    }
   ],
   "source": [
    "context_list, file, page_id= find_context_data(best_match=best_match, key_list=key_list, file_page_cont_dict=file_page_cont_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29b3d530-663b-439b-8dab-0c13b4e44b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['same direction (though smaller in amount) as changes in the rate of income The relation between the increment of consumption which has to accompany a given increment of saving is given by the marginal propensity to consume The ratio thus determined between an increment of investment and the corresponding increment of aggregate income both measured in wage units is given by the investment multiplier Finally if we assume (as a first approximation) that the employment multiplier is equal to the investment multiplier we can by applying the multiplier to the increment (or decrement) in the rate of investment brought about by the factors first described infer the increment of employment An increment (or decrement) of employment is liable however to raise (or lower) the schedule of liquidity preference there being three ways in which it will tend to increase the demand for money inasmuch as the value of output will rise when employment increases even if the wage unit and prices (in terms of the wage unit) are unchanged but in addition the wage unit itself will tend to rise as employment improves and the increase in output will be accompanied by a rise of prices (in terms of the wage unit) owing to increasing cost in the short period Thus the position of equilibrium will be influenced by these repercussions and there are other repercussions also Moreover there is not one of the above factors which is not liable to change without much warning and sometimes substantially Hence the extreme complexity of the actual course of events Nevertheless these seem to be the factors which it is useful and convenient to isolate If we examine any actual problem along the lines of the above schematism we shall find it more manageable and our practical intuition (which can take account of a more detailed complex of facts than can be treated on general principles) will be offered a less intractable material upon which to work III The above is a summary of the General Theory But the actual phenomena of the economic system are also coloured by certain special characteristics of the propensity to consume the schedule of the marginal efficiency of capital and the rate of interest about which we can safely generalise from experience but which are not logically necessary In particular it is an outstanding characteristic of the economic system in which we live that whilst it is subject to severe fluctuations in respect of output and employment it is not violently unstable Indeed it seems capable of remaining in a chronic condition of subnormal activity for a considerable period without any marked tendency either towards recovery or towards complete collapse Moreover the evidence indicates that full or even approximately full employment is of rare and short lived occurrence Fluctuations may start briskly but seem to wear themselves out before they have proceeded to great extremes and an intermediate situation which is neither desperate nor satisfactory is our normal lot It is upon the fact that fluctuations tend to wear themselves out before proceeding to extremes and eventually to reverse themselves that the theory of business cycles having a regular phase has been founded The same thing is true of prices which ']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4251d3-162c-4d2a-bdd1-a4b79ebbb673",
   "metadata": {},
   "source": [
    "## LLM Retrieval-Augmented Answer Generation (RAAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34c7695-d7cc-4613-b869-3fec378d3cdf",
   "metadata": {},
   "source": [
    "Once we have the appropriate context we can start using `LangChain` and `ctransformers` in order to initialize our **Large Language Model (LLM)**, the **Falcon-40b-Instruct Open Assistant Model** in our case. Then we set up a **prompt template which contains variables in curly brackets `{}`** and pass in the found context via our **Semantic Search**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "855caf63-19dd-4fb6-86d0-024a676fb32b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from langchain.llms import LlamaCpp\n",
    "#falcon_model = r\"C:\\Users\\svens\\02_DataScience\\00_ML\\NLP\\openModels\\falcon-40b-top1-560.ggccv1.q4_k.bin\"\n",
    "mistral_model = r\"C:\\Users\\svens\\02_DataScience\\00_ML\\NLP\\openModels\\mistral-7b-instruct-v0.1.Q8_0.gguf\"\n",
    "#llm = AutoModelForCausalLM.from_pretrained(falcon_model, model_type='falcon', threads=8, context_length=2048, max_new_tokens=1024)\n",
    "llm_config = {'max_new_tokens': 1024, 'context_length': 5000, 'gpu_layers': 5, 'threads': 8}\n",
    "llm = CTransformers(model=mistral_model, model_type=\"mistral\", config=llm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f44872c-3faa-440b-8872-8e6c1fed335b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now accelerate and send llm to gpu\n",
    "accelerator = Accelerator()\n",
    "llm, llm_config = accelerator.prepare(llm, llm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf8a2a3a-683b-4e47-af48-fd560fb4f8f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['context', 'query']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now make a use case specific template for your prompt\n",
    "prompt_template = \"\"\"Use the following given economic context in order to answer concisely the given query at the end. If you do not know the answer, please simply say that you do not know it and do not try to make up an answer.\n",
    "{context}\n",
    "Query: {query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d49ae7be-f486-403f-90b6-0446ad860b75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The theory of business cycles explains the periodic fluctuations in economic activity, known as business cycles, that occur in capitalist economies. These fluctuations are caused by a combination of factors, including changes in consumer spending, investment, and interest rates. While business cycles can be severe and unstable, they tend to wear themselves out before reaching extremes and eventually reverse themselves. The theory of business cycles is based on the idea that these fluctuations are not only common but also predictable and cyclical in nature.\n"
     ]
    }
   ],
   "source": [
    "# Use the LLM to generate an answer from the given context\n",
    "query_llm = LLMChain(llm=llm, prompt=prompt)\n",
    "response = query_llm.run({\"context\": str(context_list[0][2000:]), \"query\": query})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a8e6d-a448-4dd1-b1dd-da75b30b5f03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
